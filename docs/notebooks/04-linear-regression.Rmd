---
title: "Linear Regression"
output:
  html_document:
    toc: yes
    toc_float: true
    css: style.css
bibliography: [references.bib, packages.bib]
---

<br>

```{r 04-setup, include=FALSE}

# Set global knitr chunk options
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, 
                      collapse = TRUE, fig.align = 'center')

library(reticulate)
use_virtualenv("/Users/b294776/Desktop/Workspace/Projects/misk/misk-homl/venv", required = TRUE)

# Set the graphical theme
ggplot2::theme_set(ggplot2::theme_light())
```

```{python, echo = FALSE}
import plotnine
plotnine.themes.theme_set(new=plotnine.themes.theme_light())
```

_Linear regression_\index{linear regression}, a staple of classical statistical modeling, is one of the simplest algorithms for doing supervised learning. Though it may seem somewhat dull compared to some of the more modern statistical learning approaches described in later modules, linear regression is still a useful and widely applied statistical learning method. Moreover, it serves as a good starting point for more advanced approaches; as we will see in later modules, many of the more sophisticated statistical learning approaches can be seen as generalizations to or extensions of ordinary linear regression. Consequently, it is important to have a good understanding of linear regression before studying more complex learning methods. This module introduces linear regression with an emphasis on prediction, rather than inference. An excellent and comprehensive overview of linear regression is provided in @kutner-2005-applied. See @faraway-2016-linear for a discussion of linear regression in R (the book's website also provides Python scripts).

# Learning objectives

By the end of this module you will know how to:

- Apply and interpret simple and multiple linear regression models.
- Apply and interpret principal component regression models.
- Apply and interpret partial least squares regression models.
- Visualize and interpret feature importance of our models.

# Prerequisites {.tabset}


## `r fontawesome::fa("python")` 

```{python}
# Helper packages
import pandas as pd
import numpy as np

# Modeling packages
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from sklearn import compose
from sklearn import model_selection
from sklearn import linear_model
```

```{python}
# Ames housing data
ames = pd.read_csv("data/ames.csv")

# create train/test split
train, test = train_test_split(ames, train_size=0.7, random_state=123)

# separate features from labels and only use numeric features
X_train = train.drop("Sale_Price", axis=1)
y_train = train[["Sale_Price"]]
```

## `r fontawesome::fa("r-project")`

```{r 04-pkgs, message=FALSE}
# Helper packages
library(dplyr)    # for data manipulation
library(ggplot2)  # for awesome graphics

# Modeling packages
library(tidymodels)

# Model interpretability packages
library(vip)      # variable importance
```

```{r 04-ames-train, message=FALSE}
# Ames housing data
ames <- AmesHousing::make_ames()

# create training/test split
set.seed(123)
split  <- initial_split(ames, prop = 0.7, strata = "Sale_Price")
ames_train  <- training(split)
ames_test   <- testing(split)
```


# Simple linear regression

Pearson's correlation coefficient is often used to quantify the strength of the linear association between two continuous variables. In this section, we seek to fully characterize that linear relationship. _Simple linear regression_ (SLR) assumes that the statistical relationship between two continuous variables (say $X$ and $Y$) is (at least approximately) linear:

\begin{equation}
  Y_i = \beta_0 + \beta_1 X_i + \epsilon_i, \quad \text{for } i = 1, 2, \dots, n,
\end{equation}

where $Y_i$ represents the _i_-th response value, $X_i$ represents the _i_-th feature value, $\beta_0$ and $\beta_1$ are fixed, but unknown constants (commonly referred to as coefficients or parameters) that represent the intercept and slope of the regression line, respectively, and $\epsilon_i$ represents noise or random error. In this module, we'll assume that the errors are normally distributed with mean zero and constant variance $\sigma^2$, denoted $\stackrel{iid}{\sim} \left(0, \sigma^2\right)$. Since the random errors are centered around zero (i.e., $E\left(\epsilon\right) = 0$), linear regression is really a problem of estimating a _conditional mean_:

\begin{equation}
  E\left(Y_i | X_i\right) = \beta_0 + \beta_1 X_i.
\end{equation}

For brevity, we often drop the conditional piece and write $E\left(Y | X\right) = E\left(Y\right)$. Consequently, the interpretation of the coefficients is in terms of the average, or mean response. For example, the intercept $\beta_0$ represents the average response value when $X = 0$ (it is often not meaningful or of interest and is sometimes referred to as a _bias term_). The slope $\beta_1$ represents the increase in the average response per one-unit increase in $X$ (i.e., it is a _rate of change_).

## Best fit line  {.tabset}

Ideally, we want estimates of $\beta_0$ and $\beta_1$ that give us the "best fitting" line. But what is meant by "best fitting"? The most common approach is to use the method of _least squares_\index{least squares} (LS) estimation; this form of linear regression is often referred to as ordinary least squares (OLS) regression. There are multiple ways to measure "best fitting", but the LS criterion finds the "best fitting" line by minimizing the _residual sum of squares_\index{residual sum of squares} (RSS):

\begin{equation}
  RSS\left(\beta_0, \beta_1\right) = \sum_{i=1}^n\left[Y_i - \left(\beta_0 + \beta_1 X_i\right)\right]^2 = \sum_{i=1}^n\left(Y_i - \beta_0 - \beta_1 X_i\right)^2.
\end{equation}

The LS estimates of $\beta_0$ and $\beta_1$ are denoted as $\widehat{\beta}_0$ and $\widehat{\beta}_1$, respectively. Once obtained, we can generate predicted values, say at $X = X_{new}$, using the estimated regression equation:

\begin{equation}
  \widehat{Y}_{new} = \widehat{\beta}_0 + \widehat{\beta}_1 X_{new},
\end{equation}

where $\widehat{Y}_{new} = \widehat{E\left(Y_{new} | X = X_{new}\right)}$ is the estimated mean response at $X = X_{new}$.

With the Ames housing data, suppose we wanted to model a linear relationship between the total above ground living space of a home (`Gr_Liv_Area`) and sale price (`Sale_Price`). We can perform an OLS regression model in Python and R with the following: 

### `r fontawesome::fa("python")`

```{python}
# create linear regression model object
lm_mod = linear_model.LinearRegression()

# fit linear model with only Gr_Liv_Area feature
lm_fit = lm_mod.fit(X_train[["Gr_Liv_Area"]], y_train)
```


### `r fontawesome::fa("r-project")`

```{r 04-model1}
# create linear regression model object
lm_mod <- linear_reg() %>%  set_engine("lm")

# fit linear model with only Gr_Liv_Area feature
lm_fit <- 
  lm_mod %>% 
  fit(Sale_Price ~ Gr_Liv_Area, data = ames_train)
```


## Visualizing the best fit line

The fitted model (`lm_fit`) is displayed in the left plot below where the points represent the values of `Sale_Price` in the training data. In the right plot, the vertical lines represent the individual errors, called _residuals_\index{residuals}, associated with each observation. The OLS criterion identifies the "best fitting" line that minimizes the sum of squares of these residuals.

```{r 04-visualize-model1, eval=TRUE, fig.width=10, fig.height=3.5, echo=FALSE, fig.cap="Figure: The least squares fit from regressing sale price on living space for the the Ames housing data. Left: Fitted regression line. Right: Fitted regression line with vertical grey bars representing the residuals."}

model1 <- lm(Sale_Price ~ Gr_Liv_Area, data = ames_train)

# Fitted regression line (full training data)
p1 <- model1 %>%
  broom::augment() %>%
  ggplot(aes(Gr_Liv_Area, Sale_Price)) + 
  geom_point(size = 1, alpha = 0.3) +
  geom_smooth(se = FALSE, method = "lm") +
  scale_y_continuous(labels = scales::dollar) +
  ggtitle("Fitted regression line")

# Fitted regression line (restricted range)
p2 <- model1 %>%
  broom::augment() %>%
  ggplot(aes(Gr_Liv_Area, Sale_Price)) + 
  geom_segment(aes(x = Gr_Liv_Area, y = Sale_Price,
                   xend = Gr_Liv_Area, yend = .fitted), 
               alpha = 0.3) +
  geom_point(size = 1, alpha = 0.3) +
  geom_smooth(se = FALSE, method = "lm") +
  scale_y_continuous(labels = scales::dollar) +
  ggtitle("Fitted regression line (with residuals)")

# Side-by-side plots
grid.arrange(p1, p2, nrow = 1,
             top = grid::textGrob("The least squares fit from regressing sale price on living space for the the Ames housing data.\n"))
```

## Coefficients  {.tabset}

The estimated coefficients from our models are shown in the code chunks below. Recall that there are slight differences between R & Python because the test/ & train data splits differ between the two instances. However, the results are very similar with our intercept ( $\widehat{\beta}_0$ ) equaling around 15,800 and the coefficient for the `Gr_Liv_Area` variable ( $\widehat{\beta}_1$ ) equaling approximately 110. To interpret, we estimate that the mean selling price of a home increases by about \$110 for each additional one square foot of above ground living space. This simple description of the relationship between the sale price and square footage using a single number (i.e. the slope) is what makes linear regression such an intuitive and popular modeling tool.


### `r fontawesome::fa("python")`

```{python}
# intercept
lm_fit.intercept_
```

```{python}
# coefficient for Gr_Liv_Area
lm_fit.coef_
```


### `r fontawesome::fa("r-project")`

```{r}
lm_fit
```


# Multiple linear regression

In practice, we often have more than one predictor. For example, with the Ames housing data, we may wish to understand if above ground square footage (`Gr_Liv_Area`) and the year the house was built (`Year_Built`) are (linearly) related to sale price (`Sale_Price`). We can extend the SLR model so that it can directly accommodate multiple predictors; this is referred to as the _multiple linear regression_ (MLR) model. With two predictors, the MLR model becomes: 

\begin{equation}
  Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \epsilon,
\end{equation}

where $X_1$ and $X_2$ are features of interest. In our Ames housing example, $X_1$ represents `Gr_Liv_Area` and $X_2$ represents `Year_Built`. 

## Fitting an MLR  {.tabset}

We can fit an MLR with Python and R as follows.

### `r fontawesome::fa("python")`

```{python}
# create linear regression model object
lm_mod = linear_model.LinearRegression()

# fit linear model with only Gr_Liv_Area and Year_Built feature
lm_fit = lm_mod.fit(X_train[["Gr_Liv_Area", "Year_Built"]], y_train)
```


### `r fontawesome::fa("r-project")`

```{r}
# create linear regression model object
lm_mod <- linear_reg() %>%  set_engine("lm")

# fit linear model with only Gr_Liv_Area and Year_Built feature
lm_fit <- 
  lm_mod %>% 
  fit(Sale_Price ~ Gr_Liv_Area + Year_Built, data = ames_train)
```

## Coefficients  {.tabset}

The LS estimates of the regression coefficients for `Gr_Liv_Area` ($\widehat{\beta}_1$) is approximately 94 and for `Year_Built` ($\widehat{\beta}_2$) it is approximately 1090 (the estimated intercept is around -2100000. In other words, every one square foot increase to above ground square footage is associated with an additional \$94 in __mean selling price__ when holding the year the house was built constant. Likewise, for every year newer a home is there is approximately an increase of \$1,090 in selling price when holding the above ground square footage constant.

```{block, type = "note"}
Recall the slight difference in coefficient values is due to the fact that we are using a different training set between R and Python.
```

### `r fontawesome::fa("python")`

```{python}
# intercept
lm_fit.intercept_
```

```{python}
# coefficients for Gr_Liv_Area and Year_Built
lm_fit.coef_
```


### `r fontawesome::fa("r-project")`

```{r}
lm_fit
```


## Interactions  {.tabset}

A contour plot of our most recent fitted regression surface is displayed in the left side of the figure below. Note how the fitted regression surface is flat (i.e., it does not twist or bend). This is true for all linear models that include only _main effects_ (i.e., terms involving only a single predictor). One way to model curvature is to include _interaction effects_. An interaction occurs when the effect of one predictor on the response depends on the values of other predictors. In linear regression, interactions can be captured via products of features (i.e., $X_1 \times X_2$). A model with two main effects can also include a two-way interaction. For example, to include an interaction between $X_1 =$ `Gr_Liv_Area` and $X_2 =$ `Year_Built`, we introduce an additional product term:

\begin{equation}
  Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1 X_2 + \epsilon.
\end{equation}

```{r 04-mlr-fit, echo=FALSE, fig.width=10, fig.height=4.5, fig.cap="Figure: In a three-dimensional setting, with two predictors and one response, the least squares regression line becomes a plane. The 'best-fit' plane minimizes the sum of squared errors between the actual sales price (individual dots) and the predicted sales price (plane)."}
# Fitted models
fit1 <- lm(Sale_Price ~ Gr_Liv_Area + Year_Built, data = ames_train)
fit2 <- lm(Sale_Price ~ Gr_Liv_Area * Year_Built, data = ames_train)

# Regression plane data
plot_grid <- expand.grid(
  Gr_Liv_Area = seq(from = min(ames_train$Gr_Liv_Area), to = max(ames_train$Gr_Liv_Area), 
                    length = 100), 
  Year_Built = seq(from = min(ames_train$Year_Built), to = max(ames_train$Year_Built), 
                   length = 100)
)
plot_grid$y1 <- predict(fit1, newdata = plot_grid)
plot_grid$y2 <- predict(fit2, newdata = plot_grid)

# Level plots
p1 <- ggplot(plot_grid, aes(x = Gr_Liv_Area, y = Year_Built, 
                            z = y1, fill = y1)) +
  geom_tile() +
  geom_contour(color = "white") +
  viridis::scale_fill_viridis(name = "Predicted\nvalue", option = "inferno") +
  theme_bw() +
  ggtitle("Main effects only")
p2 <- ggplot(plot_grid, aes(x = Gr_Liv_Area, y = Year_Built, 
                            z = y2, fill = y1)) +
  geom_tile() +
  geom_contour(color = "white") +
  viridis::scale_fill_viridis(name = "Predicted\nvalue", option = "inferno") +
  theme_bw() +
  ggtitle("Main effects with two-way interaction")
gridExtra::grid.arrange(p1, p2, nrow = 1)
```

```{block, type = "note"}
Interaction effects are quite prevalent in predictive modeling. Since linear models are an example of parametric modeling, it is up to the analyst to decide if and when to include interaction effects. In later modules, we'll discuss algorithms that can automatically detect and incorporate interaction effects (albeit in different ways). It is also important to understand a concept called the ***hierarchy principle***---which demands that all lower-order terms corresponding to an interaction be retained in the model---when considering interaction effects in linear regression models.
```

In R and Python we can fit models with an interaction effect in the following manner:

### `r fontawesome::fa("python")`

```{python}
# create linear regression model object
lm_mod = linear_model.LinearRegression()

# use PolynomialFeatures to create main Gr_Liv_Area and Year_Built effects and
# also an interaction effect between Gr_Liv_Area & Year_Built
effects = preprocessing.PolynomialFeatures(
  interaction_only=True,
  include_bias=False
  )
features = effects.fit_transform(X_train[["Gr_Liv_Area", "Year_Built"]])

# fit linear model with only Gr_Liv_Area and Year_Built feature and
# also include an interaction effect (Gr_Liv_Area:Year_Built)
lm_fit = lm_mod.fit(features, y_train)
```

```{python}
# coefficients for Gr_Liv_Area, Year_Built effects and the interaction 
# effect between Gr_Liv_Area & Year_Built
lm_fit.coef_
```

### `r fontawesome::fa("r-project")`

```{r}
# create linear regression model object
lm_mod <- linear_reg() %>%  set_engine("lm")

# fit linear model with Gr_Liv_Area and Year_Built main effects and
# also include an interaction effect (Gr_Liv_Area:Year_Built)
lm_fit <- 
  lm_mod %>% 
  fit(
    Sale_Price ~ Gr_Liv_Area + Year_Built + Gr_Liv_Area:Year_Built, 
    data = ames_train
    )

lm_fit
```

## Many features  {.tabset}

In general, we can include as many predictors as we want, as long as we have more rows than parameters! The general multiple linear regression model with _p_ distinct predictors is

\begin{equation}
  Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_p X_p + \epsilon,
\end{equation}

where $X_i$ for $i = 1, 2, \dots, p$ are the predictors of interest. Note some of these may represent interactions (e.g., $X_3 = X_1 \times X_2$) between our transformations^[Transformations of the features serve a number of purposes (e.g., modeling nonlinear relationships or alleviating departures from common regression assumptions). See @kutner-2005-applied for details.] (e.g., $X_4 = \sqrt{X_1}$) of the original features. Unfortunately, visualizing beyond three dimensions is not practical as our best-fit plane becomes a hyperplane. However, the motivation remains the same where the best-fit hyperplane is identified by minimizing the RSS. 

So say we want to run an MLR model with all the features in our Ames data set as main effects (i.e., no interaction terms) to predict `Sale_Price`. The following allows us to do so:

### `r fontawesome::fa("python")`

As discussed in the feature engineering module, Python does not automatically handle categorical features. Consequently, to use all the features in the Ames housing data we need to preprocess the categorical features to turn them into numeric representations. The following simply dummy encodes all non-numeric features and then uses this transformed feature set in an MLR model.

```{python}
# create new feature set with categorical features dummy encoded
encoder = preprocessing.OneHotEncoder(drop='first')
cat_feat_only = compose.make_column_selector(dtype_include="object")
preprocessor = compose.ColumnTransformer(
  remainder="passthrough",
  transformers=[("one-hot", encoder, cat_feat_only)]
  )
X_train_encoded = preprocessor.fit_transform(X_train)

# MLR model with new dummy encoded feature set
lm_mod = linear_model.LinearRegression()
lm_fit = lm_mod.fit(X_train_encoded, y_train)
```

We can see the first 10 coefficients as follows:

```{python}
lm_fit.coef_[0, 0:10]
```

### `r fontawesome::fa("r-project")`

In R, the shorthand notation to include all features is `target_variable ~ .`. Behind the scenes, R will automatically dummy encode all categorical features. Since there are a lot of features we can use `tidy` to summarize the results in a more predictable and useful format (e.g. a data frame with standard column names). The "estimate" column represents the estimated coefficients.

```{r}
# create linear regression model object
lm_mod <- linear_reg() %>%  set_engine("lm")

# fit linear model with all features
lm_fit <- 
  lm_mod %>% 
  fit(Sale_Price ~ ., data = ames_train)

tidy(lm_fit)
```

# Assessing model accuracy {.tabset}

We've fit three main effects models to the Ames housing data: a single predictor, two predictors, and all possible predictors. But the question remains, which model is "best"? To answer this question we have to define what we mean by "best". In our case, we'll use the RMSE metric along with cross-validation to determine the "best" model. 

## `r fontawesome::fa("python")`

To compare the same model type across multiple different feature combinations we:

1. create the different feature sets (we reuse the dummy encoded feature set from the earlier section),
2. define our loss function and resampling procedure,
3. initialize an empty object to hold our results, and
4. iterate over each feature set and apply a cross validated linear regression model.

```{python}
# feature sets to compare across
feature_set1 = X_train[["Gr_Liv_Area"]]
feature_set2 = X_train[["Gr_Liv_Area", "Year_Built"]]
feature_set3 = X_train_encoded
feature_sets = {'lm1': feature_set1, 'lm2': feature_set2, 'lm3': feature_set3}

# define loss function
loss = 'neg_root_mean_squared_error'

# create 10 fold CV object
kfold = model_selection.KFold(n_splits=10, random_state=8451, shuffle=True)

# object to store CV RMSE results
results = {}

for name, feat in feature_sets.items():
  # create LM model object
  lm_mod = linear_model.LinearRegression()

  # execute and score the cross validation procedure
  cv_results = model_selection.cross_val_score(
    estimator=lm_mod, 
    X=feat, 
    y=y_train, 
    cv=kfold, 
    scoring=loss
    )
  results[name] = np.absolute(cv_results.mean())
```

We can see that as we add additional features our CV RMSE decreases.

```{python}
results
```


## `r fontawesome::fa("r-project")`

To compare different model specifications in R we:

1. initialize our regression model object,
2. create our three different model recipes, and then
3. combine them into a workflow object
 ```{r}
 # create linear regression model object
 lm_mod <- linear_reg() %>%  set_engine("lm")
 
 # create three model recipes
 lm1 <- recipe(Sale_Price ~ Gr_Liv_Area, data = ames_train) %>% 
   step_dummy(all_nominal_predictors())
 lm2 <- recipe(Sale_Price ~ Gr_Liv_Area + Year_Built, data = ames_train) %>% 
   step_dummy(all_nominal_predictors())
 lm3 <- recipe(Sale_Price ~ ., data = ames_train) %>% 
   step_dummy(all_nominal_predictors())
 
 # combine model objects and recipes into a workflow object
 preproc <- list(lm1, lm2, lm3)
 models <- list(lm_mod)
 
 model_set <- workflow_set(preproc, models, cross = TRUE)
 model_set
 ```
4. create our k-fold CV object and
5. iterate over our workflow object to execute and score the cross validation procedure

```{r, message=FALSE, warning=FALSE}
# create our k-fold CV object
kfold <- vfold_cv(ames_train, v = 10)

# iterate over our workflow object to execute and score the cross 
# validation procedure
lm_models <- model_set %>%
  workflow_map("fit_resamples",
               seed = 8451,
               resamples = kfold)

lm_models
```

We can now access the CV RMSE, which is in the "mean" column. We can see that as we add additional features our CV RMSE decreases.

```{r}
collect_metrics(lm_models) %>% 
  filter(.metric == "rmse")
```

```{block, type = "tip"}
R's workflow procedure may seem daunting at first but it provides a lot of flexibility once you become comfortable with the concept. Read more about workflows here: https://www.tmwr.org/workflows.html
```

# Principal component regression

TBD

# Exercises

TBD

[🏠](https://github.com/misk-data-science/misk-homl)

# References

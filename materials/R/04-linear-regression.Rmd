---
title: "Linear Regression & Cousins"
output: html_notebook
---

# Prereqs 

```{r slide-3}
# packages required
library(dplyr)
library(ggplot2)
library(rsample)
library(recipes)
library(vip)
library(caret)

# ames data
ames <- AmesHousing::make_ames()

# split data
set.seed(123)
split <- initial_split(ames, strata = "Sale_Price")
ames_train <- training(split)
```

# Simple linear regression 

- `lm()` performs OLS in base R
- `glm()` also performs linear regression but extends to other generalized methods (i.e. logistic regression)
- `summary(model)` provides many results (i.e. "Residual Standard Error" is the RMSE)
- No method for resampling (i.e. cross validation) with `lm()`

```{r slide-6}
model1 <- lm(Sale_Price ~ Gr_Liv_Area, data = ames_train)
summary(model1)
```

# Multiple linear regression 

```{r slide-7}
# OLS model with two predictors
model2 <- lm(Sale_Price ~ Gr_Liv_Area + Year_Built, data = ames_train)

# OLS model with specified interactions
model3 <- lm(Sale_Price ~ Gr_Liv_Area + Year_Built + Gr_Liv_Area:Year_Built, data = ames_train)
# Equivalent to Gr_Liv_Area*Year_Built

# include all possible main effects
model4 <- lm(Sale_Price ~ ., data = ames_train)
```

```{r eval = FALSE}
summary(model4)
```

# Assessing model accuracy 

We've fit four models to the Ames housing data: 

1. a single predictor, 
2. two predictors, 
3. two predictors with interaction,
4. and all possible main effect predictors. 

___Which model is "best"?___

```{r slide-9}
# create a resampling method
cv <- trainControl(
  method = "repeatedcv", 
  number = 10, 
  repeats = 5
  )

# model 1 CV
set.seed(123)
(cv_model1 <- train(
  Sale_Price ~ Gr_Liv_Area, 
  data = ames_train, 
  method = "lm", #<<
  trControl = cv)
)
```

Model using most predictors is marginally superior

```{r slide-10}
# model 2 CV
set.seed(123)
cv_model2 <- train(
  Sale_Price ~ Gr_Liv_Area + Year_Built, 
  data = ames_train, 
  method = "lm",
  trControl = cv
  )

# model 3 CV
set.seed(123)
cv_model3 <- train(
  Sale_Price ~ Gr_Liv_Area + Year_Built + Gr_Liv_Area : Year_Built, 
  data = ames_train, 
  method = "lm",
  trControl = cv
  )

# model 4 CV
set.seed(123)
cv_model4 <- train(
  Sale_Price ~ ., 
  data = ames_train, 
  method = "lm",
  trControl = cv
  )

# Extract out of sample performance measures
summary(resamples(list(
  model1 = cv_model1, 
  model2 = cv_model2, 
  model3 = cv_model3,
  model4 = cv_model4
)))
```

# Model concerns - multicollinearity


```{r slide-16}
m1 <- lm(Sale_Price ~ Gr_Liv_Area + TotRms_AbvGrd, data = ames_train) # 2 predictors
m2 <- lm(Sale_Price ~ Gr_Liv_Area, data = ames_train)     # only predictor 1
m3 <- lm(Sale_Price ~ TotRms_AbvGrd, data = ames_train)   # only predictor 1

coef(m1) 
coef(m2) 
coef(m3) 
```


```{r}
# The are highly correlated
cor(ames_train$Gr_Liv_Area, ames_train$TotRms_AbvGrd )

# simple plot
plot(ames_train$Gr_Liv_Area, ames_train$TotRms_AbvGrd )
```

```{r}

# # which(names(ames_train) == "Sale_Price")
# library(ggcorrplot)
# corr <- round(cor(ames_train[-79]), 1)
# ggcorrplot(corr)
```


# Principal Component Regression

Feature reduction with PCR improves prediction error by ~ $10K

```{r slide-21}
# 1. hypergrid
hyper_grid <- expand.grid(ncomp = seq(2, 40, by = 2))

# 2. PCR
set.seed(123)
cv_pcr <- train(
  Sale_Price ~ ., 
  data = ames_train, 
  trControl = cv,
  method = "pcr", #<<
  preProcess = c("zv", "center", "scale"), #<<
  tuneGrid = hyper_grid, #<<
  metric = "RMSE"
  )

# model with lowest RMSE
cv_pcr$bestTune

cv_pcr$results %>%
  filter(ncomp == as.numeric(cv_pcr$bestTune))

# plot cross-validated RMSE
plot(cv_pcr)
```


Tuning

- The number of PCs is the only hyperparameter
- rule of thumb
   - assess 2-*p* in evenly divided segments
   - start with a few and zoom in


```{r slide-22}
# 1. hypergrid
p <- length(ames_train) - 1
hyper_grid <- expand.grid(ncomp = seq(2, 80, length.out = 10)) #<<

# 2. PCR
set.seed(123)
cv_pcr <- train(
  Sale_Price ~ ., 
  data = ames_train, 
  trControl = cv,
  method = "pcr", 
  preProcess = c("zv", "center", "scale"), 
  tuneGrid = hyper_grid, 
  metric = "RMSE"
  )

# RMSE
cv_pcr$results %>%
  filter(ncomp == cv_pcr$bestTune$ncomp)

# plot cross-validated RMSE
plot(cv_pcr)
```


# Partial Least Squares Regression

Using PLS improves prediction error by an additional $500

```{r slide-27}
# PLS
set.seed(123)
cv_pls <- train(
  Sale_Price ~ ., 
  data = ames_train, 
  trControl = cv,
  method = "pls", #<<
  preProcess = c("zv", "center", "scale"),
  tuneGrid = hyper_grid,
  metric = "RMSE"
  )

# model with lowest RMSE
cv_pls$bestTune

cv_pls$results %>%
  filter(ncomp == as.numeric(cv_pls$bestTune))

# plot cross-validated RMSE
plot(cv_pls)
```


# Model Comparison

```{r slide-31}
results <- resamples(list(
  OLS  = cv_model4, 
  PCR  = cv_pcr, 
  PLS  = cv_pls
  ))

summary(results)$statistics$RMSE

# plot results
p1 <- bwplot(results, metric = "RMSE")
p2 <- dotplot(results, metric = "RMSE")
gridExtra::grid.arrange(p1, p2, nrow = 1)
```


# Feature importance 

vip Package

* variable importance plots illustrate the influence each predictor has
* many packages have their own vip plots
* the __vip__ package provides a common output
* different models measure "importance" differently
* we'll review this more indepth tomorrow

```{r slide-33}
# MARS model
vip(cv_pls)

```

```{r slide-34}
# all models
p1 <- vip(cv_model4, num_features = 25, bar = FALSE) + ggtitle("OLS")
p2 <- vip(cv_pls, num_features = 25, bar = FALSE) + ggtitle("PLS")

gridExtra::grid.arrange(p1, p2, nrow = 1)
```

# Feature effects 

* feature effects measures the relationship between a feature and the target variable
* most common approach is a partial dependence plot
* computes the average response value when all observations use a particular value for a given feature
* we will review this more tomorrow

```{r slide-35}
# linear regression model
pdp::partial(cv_model2, pred.var = "Gr_Liv_Area", grid.resolution = 10) %>% 
  autoplot()
```

```{r slide-36}
# linear regression model
p1 <- pdp::partial(cv_model2, pred.var = "Gr_Liv_Area", grid.resolution = 10) %>% 
  autoplot() +
  ggtitle("OLS with 2 features") +
  scale_y_continuous("Predicted Sales Price", 
                     labels = scales::dollar, 
                     limits = c(0, 600000))

# PLS model
p2 <- pdp::partial(cv_model4, pred.var = "Gr_Liv_Area", grid.resolution = 10) %>% 
  autoplot() +
  ggtitle("OLS with all features") +
  scale_y_continuous("Predicted Sales Price", 
                     labels = scales::dollar, 
                     limits = c(0, 600000))

# Regularized model
p3 <- pdp::partial(cv_pcr, pred.var = "Gr_Liv_Area", grid.resolution = 10) %>% 
  autoplot() +
  ggtitle("PCR") +
  scale_y_continuous("Predicted Sales Price", 
                     labels = scales::dollar, 
                     limits = c(0, 600000))

# MARS model
p4 <- pdp::partial(cv_pls, pred.var = "Gr_Liv_Area", grid.resolution = 10) %>% 
  autoplot() +
  ggtitle("PLS") +
  scale_y_continuous("Predicted Sales Price", 
                     labels = scales::dollar, 
                     limits = c(0, 600000))

gridExtra::grid.arrange(p1, p2, p3, p4, nrow = 1)
```


```{r eval = FALSE}
coef(model2)[["Gr_Liv_Area"]]
library(stringr)
length(coef(model4))
coef(model4)[213]
which(str_detect(names(coef(model4)),"Gr_Liv_Area"))
```


Assess the interaction of the top 2 predictors:

```{r slide-37}
pdp::partial(
  cv_model2,
  pred.var = c("Gr_Liv_Area", "Year_Built"),
  grid.resolution = 10
  ) %>% 
  pdp::plotPartial(
    levelplot = FALSE,
    zlab = "yhat", 
    drape = TRUE, 
    colorkey = TRUE, 
    screen = list(z = -20, x = -60)
    )
```
